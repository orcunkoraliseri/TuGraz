{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://enlight.nyc/projects/neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTakes inputs as a matrix (2D array of numbers)\\nMultiplies the input by a set weights (performs a dot product aka matrix multiplication)\\nApplies an activation function\\nReturns an output\\nError is calculated by taking the difference from the desired output from the data and the predicted output. This creates our gradient descent, which we can use to alter the weights\\nThe weights are then altered slightly according to the error.\\nTo train, this process is repeated 1,000+ times. The more the data is trained upon, the more accurate our outputs will be.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simple feedforward neural network\n",
    "'''\n",
    "Takes inputs as a matrix (2D array of numbers)\n",
    "Multiplies the input by a set weights (performs a dot product aka matrix multiplication)\n",
    "Applies an activation function\n",
    "Returns an output\n",
    "Error is calculated by taking the difference from the desired output from the data and the predicted output. This creates our gradient descent, which we can use to alter the weights\n",
    "The weights are then altered slightly according to the error.\n",
    "To train, this process is repeated 1,000+ times. The more the data is trained upon, the more accurate our outputs will be.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4 0.9]\n",
      " [0.2 0.5]\n",
      " [0.6 0.6]\n",
      " [1.  1. ]] \n",
      "\n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]] \n",
      "\n",
      " [[0.4 0.9]\n",
      " [0.2 0.5]\n",
      " [0.6 0.6]] \n",
      "\n",
      " [[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# X = (hours studying, hours sleeping), y = score on test\n",
    "xAll = np.array(([2, 9], [1, 5], [3, 6], [5, 10]), dtype=float) # input data\n",
    "y = np.array(([92], [86], [89]), dtype=float) # output\n",
    "\n",
    "# scale units between 0-1\n",
    "xAll = xAll/np.amax(xAll, axis=0) # scaling input data\n",
    "y = y/100 # scaling output data (max test score is 100)\n",
    "\n",
    "# split data\n",
    "X = np.split(xAll, [3])[0] # training data\n",
    "xPredicted = np.split(xAll, [3])[1] # testing data\n",
    "\n",
    "print(xAll,'\\n'*2, y ,'\\n'*2, X ,'\\n'*2, xPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        #parameters\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "        \n",
    "        #weights\n",
    "        self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n",
    "        self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        #activation function\n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #forward propagation\n",
    "        self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "   \n",
    "    def sigmoidPrime(self, s):\n",
    "        #derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        # backward propagate through the network\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "\n",
    "        self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "        self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "        \n",
    "    def train (self, X, y):\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def saveWeights(self):\n",
    "        np.savetxt(\"w1.txt\", self.W1, fmt=\"%s\")\n",
    "        np.savetxt(\"w2.txt\", self.W2, fmt=\"%s\")\n",
    "\n",
    "    def predict(self):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "        print (\"Output: \\n\" + str(self.forward(xPredicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[[0.4 0.9]\n",
      " [0.2 0.5]\n",
      " [0.6 0.6]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      "[[0.89545415]\n",
      " [0.87993262]\n",
      " [0.89452136]]\n",
      "Loss: \n",
      "0.0003400835687652715\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "  NN.train(X,y)\n",
    "\n",
    "print (\"Input: \\n\" + str(X))\n",
    "print (\"Actual Output: \\n\" + str(y))\n",
    "print (\"Predicted Output: \\n\" + str(NN.forward(X)))\n",
    "print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "[[1. 1.]]\n",
      "Output: \n",
      "[[0.90997025]]\n"
     ]
    }
   ],
   "source": [
    "NN.saveWeights()\n",
    "NN.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
